# app.py (Improved Version - íŒŒì¼ íƒ€ì…ë³„ ìµœì í™” ë° ì¤‘ë³µ ë°©ì§€)

import uvicorn
import os
import logging
import json
from pathlib import Path
from fastapi import FastAPI, HTTPException, BackgroundTasks
from contextlib import asynccontextmanager
import uuid
import re
from pydantic import BaseModel, Field
from typing import List
from collections import Counter
from datetime import datetime, timedelta

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import OllamaEmbeddings

# ëŒ€ì²´ import ì‹œë„
try:
    from langchain_qdrant import Qdrant
except ImportError:
    from langchain_community.vectorstores import Qdrant
    
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    from langchain_community.llms import Ollama as OllamaLLM

# Google Drive API ê´€ë ¨
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload

# ê¸°íƒ€
import io
import pandas as pd
try:
    import pypdf
except ImportError:
    logging.warning("pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    pypdf = None

from qdrant_client import QdrantClient, models
from apscheduler.schedulers.background import BackgroundScheduler

# --- 1. ë¡œê¹… ë° ê¸°ë³¸ ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 2. í™˜ê²½ ë³€ìˆ˜ ë° ì£¼ìš” ì„¤ì •ê°’ ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "llama3.2")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.2")
COLLECTION_NAME = "gdrive_rag_collection"
SERVICE_ACCOUNT_FILE = "/app/service_account.json"
GOOGLE_DRIVE_FOLDER_ID = os.getenv("GOOGLE_DRIVE_FOLDER_ID")
SYNC_INTERVAL_MINUTES = int(os.getenv("SYNC_INTERVAL_MINUTES", "60"))
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# RAG ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
CHUNK_SIZE = 2000
CHUNK_OVERLAP = 300
EMBEDDING_BATCH_SIZE = 32  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ê°ì†Œ
CHUNK_VERSION = f"{CHUNK_SIZE}_{CHUNK_OVERLAP}_optimized_v5"  # ìƒˆ ë²„ì „

# --- 3. ì „ì—­ ë³€ìˆ˜ ---
vectorstore = None
qa_chain = None
embeddings = None
scheduler = BackgroundScheduler(timezone="Asia/Seoul")
drive_service = None
sync_in_progress = False

# --- 4. Google Drive ì—°ë™ ë° íŒŒì¼ íŒŒì‹± ---
def get_drive_service():
    global drive_service
    if drive_service: 
        return drive_service
    if not Path(SERVICE_ACCOUNT_FILE).exists():
        logger.error(f"ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼({SERVICE_ACCOUNT_FILE})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
        drive_service = build('drive', 'v3', credentials=credentials)
        logger.info("Google Drive API ì„œë¹„ìŠ¤ ì¸ì¦ ì™„ë£Œ")
        return drive_service
    except Exception as e:
        logger.error(f"Google Drive API ì„œë¹„ìŠ¤ ì¸ì¦ ì‹¤íŒ¨: {e}")
        return None

def _decode_bytes_to_text(content_bytes, file_id):
    """ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ ë°”ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©"""
    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'iso-8859-1', 'latin1']
    for encoding in encodings:
        try:
            return content_bytes.decode(encoding)
        except UnicodeDecodeError: 
            continue
    logger.warning(f"íŒŒì¼ {file_id} ë””ì½”ë”© ì‹¤íŒ¨. ì¼ë¶€ ë¬¸ì ëŒ€ì²´ë¨.")
    return content_bytes.decode('utf-8', errors='replace')

def parse_excel_to_markdown(file_io, file_name):
    """ì—‘ì…€ íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ë³€í™˜"""
    try:
        # ì—”ì§„ í˜¸í™˜ì„± í™•ë³´
        try:
            xls = pd.ExcelFile(file_io, engine='openpyxl')
        except Exception:
            try:
                xls = pd.ExcelFile(file_io, engine='xlrd')
            except Exception as e:
                logger.error(f"ì—‘ì…€ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({file_name}): {e}")
                return None
                
        markdown_parts = []
        
        logger.info(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ '{file_name}' ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹œì‘. ì‹œíŠ¸ ëª©ë¡: {xls.sheet_names}")
        
        # íŒŒì¼ ë©”íƒ€ë°ì´í„°
        markdown_parts.append(f"# ğŸ“Š {file_name}\n\n")
        markdown_parts.append(f"**íŒŒì¼ í˜•ì‹:** Microsoft Excel\n")
        markdown_parts.append(f"**ì‹œíŠ¸ ìˆ˜:** {len(xls.sheet_names)}\n")
        markdown_parts.append(f"**ì‹œíŠ¸ ëª©ë¡:** {', '.join(xls.sheet_names)}\n\n")
        
        processed_sheets = 0
        
        for sheet_name in xls.sheet_names:
            try:
                df = pd.read_excel(
                    xls, 
                    sheet_name=sheet_name, 
                    header=None, 
                    dtype=str, 
                    keep_default_na=False,
                    na_filter=False
                )
                
                if df.empty or df.shape[0] == 0: 
                    logger.info(f"  - ì‹œíŠ¸ '{sheet_name}': ë¹ˆ ì‹œíŠ¸")
                    continue
                
                # ì‹œíŠ¸ í—¤ë”
                markdown_parts.append(f"## ğŸ“‹ ì‹œíŠ¸: {sheet_name}\n\n")
                
                # ë¹ˆ í–‰ ì œê±°
                non_empty_rows = []
                for idx, row in df.iterrows():
                    if any(str(cell).strip() and str(cell).lower() not in ['nan', 'none', 'null', ''] for cell in row):
                        non_empty_rows.append(idx)
                
                if not non_empty_rows:
                    markdown_parts.append("*ë°ì´í„°ê°€ ì—†ëŠ” ì‹œíŠ¸*\n\n")
                    continue
                
                df_filtered = df.iloc[non_empty_rows].reset_index(drop=True)
                
                # í—¤ë” ê°ì§€ (ì²˜ìŒ 5í–‰ ì¤‘ ê°€ì¥ ë§ì€ ë°ì´í„°ë¥¼ ê°€ì§„ í–‰)
                header_row_idx = None
                max_non_empty = 0
                
                for idx in range(min(5, len(df_filtered))):
                    row = df_filtered.iloc[idx]
                    non_empty_count = sum(
                        1 for cell in row 
                        if str(cell).strip() and str(cell).lower() not in ['nan', 'none', 'null', 'unnamed']
                    )
                    if non_empty_count > max_non_empty and non_empty_count >= 2:
                        max_non_empty = non_empty_count
                        header_row_idx = idx
                
                if header_row_idx is not None:
                    # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    header_row = df_filtered.iloc[header_row_idx]
                    headers = []
                    
                    for i, cell in enumerate(header_row):
                        cell_str = str(cell).strip()
                        if cell_str and cell_str.lower() not in ['nan', 'none', 'null', 'unnamed']:
                            clean_header = cell_str.replace('|', '\\|').replace('\n', ' ').replace('\r', '')
                            headers.append(clean_header)
                        else:
                            headers.append(f"ì»¬ëŸ¼{i+1}")
                    
                    # ìµœëŒ€ ì»¬ëŸ¼ ìˆ˜ ì œí•œ
                    max_columns = min(len(headers), 10)
                    headers = headers[:max_columns]
                    
                    # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í—¤ë”
                    markdown_parts.append("| " + " | ".join(headers) + " |\n")
                    markdown_parts.append("| " + " | ".join(["---"] * len(headers)) + " |\n")
                    
                    # ë°ì´í„° í–‰ë“¤
                    data_rows_added = 0
                    max_rows = 100
                    
                    for row_idx in range(header_row_idx + 1, len(df_filtered)):
                        if data_rows_added >= max_rows:
                            markdown_parts.append(f"| ... | *({len(df_filtered) - header_row_idx - 1 - max_rows}ê°œ í–‰ ë” ìˆìŒ)* | ... |\n")
                            break
                            
                        row = df_filtered.iloc[row_idx]
                        row_data = []
                        
                        for i in range(len(headers)):
                            if i < len(row):
                                cell = row.iloc[i]
                                cell_str = str(cell).strip()
                                if cell_str and cell_str.lower() not in ['nan', 'none', 'null']:
                                    clean_cell = (cell_str.replace('|', '\\|')
                                                        .replace('\n', '<br>')
                                                        .replace('\r', ''))
                                    if len(clean_cell) > 100:
                                        clean_cell = clean_cell[:97] + "..."
                                    row_data.append(clean_cell)
                                else:
                                    row_data.append("")
                            else:
                                row_data.append("")
                        
                        if any(cell.strip() for cell in row_data):
                            markdown_parts.append("| " + " | ".join(row_data) + " |\n")
                            data_rows_added += 1
                
                else:
                    # í—¤ë”ê°€ ì—†ëŠ” ê²½ìš° ë¦¬ìŠ¤íŠ¸ í˜•ì‹
                    markdown_parts.append("### ğŸ“ ë°ì´í„° (êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í˜•ì‹)\n\n")
                    
                    rows_added = 0
                    max_list_rows = 50
                    
                    for row_idx, row in df_filtered.iterrows():
                        if rows_added >= max_list_rows:
                            markdown_parts.append(f"*...ë° {len(df_filtered) - max_list_rows}ê°œ í–‰ ë”*\n\n")
                            break
                            
                        row_items = []
                        for cell in row:
                            cell_str = str(cell).strip()
                            if cell_str and cell_str.lower() not in ['nan', 'none', 'null']:
                                if len(cell_str) > 50:
                                    cell_str = cell_str[:47] + "..."
                                row_items.append(cell_str)
                        
                        if row_items:
                            markdown_parts.append(f"- **í–‰ {rows_added + 1}:** {' | '.join(row_items)}\n")
                            rows_added += 1
                
                markdown_parts.append("\n")
                processed_sheets += 1
                logger.info(f"  âœ… ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as sheet_error:
                logger.error(f"ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {sheet_error}")
                markdown_parts.append(f"*âŒ ì‹œíŠ¸ '{sheet_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(sheet_error)[:100]}*\n\n")
                continue
        
        if processed_sheets == 0:
            logger.warning(f"ì—‘ì…€ íŒŒì¼ '{file_name}'ì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì‹œíŠ¸ê°€ ì—†ìŒ")
            return None
            
        final_content = "".join(markdown_parts)
        logger.info(f"ğŸ“Š ì—‘ì…€ ë³€í™˜ ì™„ë£Œ: {processed_sheets}ê°œ ì‹œíŠ¸, {len(final_content)}ì")
        return final_content
        
    except Exception as e:
        logger.error(f"ì—‘ì…€ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì˜¤ë¥˜ ({file_name}): {e}", exc_info=True)
        return None

def parse_pdf_to_text(file_io, file_name):
    """PDFë¥¼ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë§ˆí¬ë‹¤ìš´ ìµœì†Œí™”)"""
    if not pypdf:
        logger.error("PDF ì²˜ë¦¬ë¥¼ ìœ„í•´ pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return None
    
    try:
        pdf_reader = pypdf.PdfReader(file_io)
        parts = [f"ë¬¸ì„œëª…: {file_name}\n"]
        
        logger.info(f"ğŸ“„ PDF íŒŒì¼ '{file_name}' ì²˜ë¦¬ ì‹œì‘. ì´ {len(pdf_reader.pages)}í˜ì´ì§€")
        
        # PDF ë©”íƒ€ë°ì´í„° (ê°„ë‹¨í•˜ê²Œ)
        if hasattr(pdf_reader, 'metadata') and pdf_reader.metadata:
            doc_meta = pdf_reader.metadata
            if doc_meta.get('/Title'):
                parts.append(f"ì œëª©: {doc_meta['/Title']}\n")
            if doc_meta.get('/Author'):
                parts.append(f"ì‘ì„±ì: {doc_meta['/Author']}\n")
            if doc_meta.get('/Subject'):
                parts.append(f"ì£¼ì œ: {doc_meta['/Subject']}\n")
        
        parts.append(f"ì´ í˜ì´ì§€: {len(pdf_reader.pages)}\n\n")
        
        # í˜ì´ì§€ë³„ ë‚´ìš© (ê°„ë‹¨í•œ êµ¬ë¶„ìë§Œ ì‚¬ìš©)
        for i, page in enumerate(pdf_reader.pages):
            try:
                text = page.extract_text()
                if text and text.strip():
                    parts.append(f"=== í˜ì´ì§€ {i+1} ===\n")
                    cleaned_text = re.sub(r'\n\s*\n', '\n\n', text.strip())
                    cleaned_text = re.sub(r' +', ' ', cleaned_text)
                    parts.append(cleaned_text + "\n\n")
            except Exception as page_err:
                logger.warning(f"PDF í˜ì´ì§€ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {page_err}")
                continue
        
        return "".join(parts)
        
    except Exception as e:
        logger.error(f"PDF íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜ ({file_name}): {e}")
        return None

def parse_csv_to_text(file_io, file_name):
    """CSVë¥¼ ê²€ìƒ‰ ì¹œí™”ì ì¸ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        content = _decode_bytes_to_text(file_io.getvalue(), "")
        lines = content.strip().split('\n')
        
        if not lines:
            return None
            
        parts = [f"íŒŒì¼ëª…: {file_name}\n"]
        parts.append(f"ë°ì´í„° í˜•ì‹: CSV\n")
        parts.append(f"ì´ í–‰ìˆ˜: {len(lines)}\n\n")
        
        # í—¤ë” (ì²« ë²ˆì§¸ í–‰)
        if lines:
            headers = [h.strip().strip('"') for h in lines[0].split(',')]
            parts.append(f"ì»¬ëŸ¼: {', '.join(headers)}\n\n")
        
        # ë°ì´í„° ìƒ˜í”Œ (ìµœëŒ€ 20í–‰)
        parts.append("ë°ì´í„° ìƒ˜í”Œ:\n")
        for i, line in enumerate(lines[:min(20, len(lines))]):
            if line.strip():
                # CSV íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
                clean_line = line.replace('"', '').strip()
                parts.append(f"í–‰{i+1}: {clean_line}\n")
        
        if len(lines) > 20:
            parts.append(f"... ë° {len(lines) - 20}ê°œ í–‰ ë”\n")
            
        return "".join(parts)
        
    except Exception as e:
        logger.error(f"CSV íŒŒì‹± ì˜¤ë¥˜ ({file_name}): {e}")
        return None

def parse_json_to_text(file_io, file_name):
    """JSONì„ ê²€ìƒ‰ ì¹œí™”ì ì¸ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    try:
        content = _decode_bytes_to_text(file_io.getvalue(), "")
        data = json.loads(content)
        
        parts = [f"íŒŒì¼ëª…: {file_name}\n"]
        parts.append(f"ë°ì´í„° í˜•ì‹: JSON\n\n")
        
        # JSON êµ¬ì¡° ê°„ë‹¨ ë¶„ì„
        def analyze_json_structure(obj, prefix="", depth=0):
            if depth > 3:  # ê¹Šì´ ì œí•œ
                return []
            result = []
            if isinstance(obj, dict):
                for key, value in list(obj.items())[:10]:  # ìµœëŒ€ 10ê°œ í‚¤ë§Œ
                    current_path = f"{prefix}.{key}" if prefix else key
                    if isinstance(value, (dict, list)):
                        result.append(f"{current_path}: {type(value).__name__}")
                        if depth < 2:  # ê¹Šì´ 2ê¹Œì§€ë§Œ
                            result.extend(analyze_json_structure(value, current_path, depth + 1))
                    else:
                        value_str = str(value)[:100]
                        result.append(f"{current_path}: {value_str}")
            elif isinstance(obj, list) and obj:
                result.append(f"{prefix}[ë°°ì—´ {len(obj)}ê°œ í•­ëª©]")
                if len(obj) > 0 and depth < 2:
                    result.extend(analyze_json_structure(obj[0], f"{prefix}[0]", depth + 1))
            return result[:30]  # ìµœëŒ€ 30ê°œ í•­ëª©ë§Œ
        
        structure_info = analyze_json_structure(data)
        parts.append("JSON êµ¬ì¡°:\n")
        for info in structure_info:
            parts.append(f"- {info}\n")
            
        return "".join(parts)
        
    except Exception as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜ ({file_name}): {e}")
        return None

def parse_file_content(file_io, file_name, mime_type, file_id):
    """íŒŒì¼ íƒ€ì…ë³„ ìµœì í™”ëœ ì½˜í…ì¸  íŒŒì‹±"""
    
    # 1. ì—‘ì…€ íŒŒì¼ë§Œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ë³€í™˜
    excel_mime_types = [
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',  # .xlsx
        'application/vnd.ms-excel',  # .xls
        'application/vnd.ms-excel.sheet.macroEnabled.12',  # .xlsm
        'application/vnd.ms-excel.sheet.binary.macroEnabled.12'  # .xlsb
    ]
    
    if mime_type in excel_mime_types:
        logger.info(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ â†’ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë³€í™˜: {file_name}")
        return parse_excel_to_markdown(file_io, file_name)

    # 2. PDF íŒŒì¼ â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸
    elif mime_type == 'application/pdf':
        logger.info(f"ğŸ“„ PDF íŒŒì¼ â†’ í…ìŠ¤íŠ¸ ë³€í™˜: {file_name}")
        return parse_pdf_to_text(file_io, file_name)

    # 3. CSV íŒŒì¼ â†’ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
    elif mime_type == 'text/csv':
        logger.info(f"ğŸ“Š CSV íŒŒì¼ â†’ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸: {file_name}")
        return parse_csv_to_text(file_io, file_name)
    
    # 4. JSON íŒŒì¼ â†’ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸
    elif mime_type == 'application/json':
        logger.info(f"ğŸ”§ JSON íŒŒì¼ â†’ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸: {file_name}")
        return parse_json_to_text(file_io, file_name)

    # 5. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ â†’ ì›ë³¸ ë³´ì¡´
    elif mime_type == 'text/markdown' or file_name.lower().endswith('.md'):
        logger.info(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ â†’ ì›ë³¸ ë³´ì¡´: {file_name}")
        content = _decode_bytes_to_text(file_io.getvalue(), file_id)
        return content

    # 6. ì¼ë°˜ í…ìŠ¤íŠ¸ â†’ ìµœì†Œí•œì˜ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ê°€
    elif mime_type.startswith('text/'):
        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼ â†’ ìˆœìˆ˜ í…ìŠ¤íŠ¸: {file_name}")
        content = _decode_bytes_to_text(file_io.getvalue(), file_id)
        if not content.strip():
            return None
        return f"íŒŒì¼ëª…: {file_name}\níŒŒì¼í˜•ì‹: í…ìŠ¤íŠ¸\n\n{content}"

    # 7. ê¸°íƒ€ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹
    else:
        logger.warning(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” MIME íƒ€ì…: {mime_type} ({file_name})")
        return None

def download_and_parse_file(service, file_id, file_name, mime_type):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±"""
    request = None
    
    # Google DocsëŠ” í…ìŠ¤íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°
    if mime_type == 'application/vnd.google-apps.document':
        request = service.files().export_media(fileId=file_id, mimeType='text/plain')
    elif mime_type in [
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        'application/vnd.ms-excel', 
        'application/vnd.ms-excel.sheet.macroEnabled.12',
        'application/vnd.ms-excel.sheet.binary.macroEnabled.12',
        'application/pdf',
        'text/csv',
        'application/json'
    ] or mime_type.startswith('text/'):
        request = service.files().get_media(fileId=file_id)
    else:
        logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {mime_type} ({file_name})")
        return None
        
    try:
        file_io = io.BytesIO()
        downloader = MediaIoBaseDownload(file_io, request)
        done = False
        while not done: 
            _, done = downloader.next_chunk()
        file_io.seek(0)
        
        # Google DocsëŠ” íŠ¹ë³„ ì²˜ë¦¬
        if mime_type == 'application/vnd.google-apps.document':
            content = _decode_bytes_to_text(file_io.getvalue(), file_id)
            return f"íŒŒì¼ëª…: {file_name}\níŒŒì¼í˜•ì‹: Google ë¬¸ì„œ\n\n{content}"
        else:
            return parse_file_content(file_io, file_name, mime_type, file_id)
            
    except Exception as e:
        logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨ ({file_name}): {e}")
        return None

# --- 5. ê°œì„ ëœ ë™ê¸°í™” ë¡œì§ ---
def perform_sync():
    """ì™„ì „íˆ ê°œì„ ëœ ë™ê¸°í™” í•¨ìˆ˜ - ì¤‘ë³µ ì—…ë¡œë“œ ì™„ì „ ë°©ì§€"""
    global embeddings, vectorstore, sync_in_progress
    
    if sync_in_progress:
        logger.info("ë™ê¸°í™”ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return
    
    sync_in_progress = True
    try:
        logger.info(f"ğŸš€ ë™ê¸°í™” ì‹œì‘")
        logger.info(f"GOOGLE_DRIVE_FOLDER_ID: {GOOGLE_DRIVE_FOLDER_ID}")
        
        if not all([embeddings, vectorstore]) or not GOOGLE_DRIVE_FOLDER_ID:
            logger.warning("RAG ì»´í¬ë„ŒíŠ¸ ë¯¸ì´ˆê¸°í™” ë˜ëŠ” í´ë” ID ëˆ„ë½ìœ¼ë¡œ ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        service = get_drive_service()
        if not service: 
            return

        # Google Driveì—ì„œ íŒŒì¼ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        try:
            query = f"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false"
            drive_files_meta = service.files().list(
                q=query, fields="files(id, name, modifiedTime, mimeType)", pageSize=1000
            ).execute().get('files', [])
            logger.info(f"Google Driveì—ì„œ {len(drive_files_meta)}ê°œ íŒŒì¼ ë©”íƒ€ë°ì´í„° í™•ì¸")
        except HttpError as e:
            logger.error(f"Google Drive API ì˜¤ë¥˜: {e}")
            return

        qdrant_client = vectorstore.client
        processed_files, skipped_files, failed_files = 0, 0, 0

        for file_meta in drive_files_meta:
            file_id, file_name, modified_time = file_meta['id'], file_meta['name'], file_meta['modifiedTime']
            
            # âœ… LangChain + Qdrant êµ¬ì¡°ì— ë§ì¶˜ ì¤‘ë³µ ì²´í¬
            try:
                existing, _ = qdrant_client.scroll(
                    collection_name=COLLECTION_NAME, 
                    limit=1, 
                    with_vectors=False,
                    scroll_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="metadata.file_id",  # âœ… LangChainì€ metadata. ì ‘ë‘ì‚¬ í•„ìš”
                                match=models.MatchValue(value=file_id)
                            )
                        ]
                    ),
                    with_payload=True
                )
                stored_payload = existing[0].payload if existing else None
            except Exception as filter_error:
                logger.warning(f"íŒŒì¼ {file_id} í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {filter_error}. ì‹ ê·œ ì²˜ë¦¬ë¡œ ì§„í–‰.")
                stored_payload = None
            
            logger.info(f"ğŸ“„ íŒŒì¼ '{file_name}' ë¶„ì„:")
            
            should_process = True
            
            if stored_payload:
                # âœ… LangChain êµ¬ì¡°ì— ë§ì¶˜ ë©”íƒ€ë°ì´í„° ì ‘ê·¼
                stored_metadata = stored_payload.get('metadata', {})
                stored_modified_time = stored_metadata.get('modified_time')
                stored_chunk_version = stored_metadata.get('chunk_version')
                
                logger.info(f"  - Qdrant ì €ì¥ëœ ìˆ˜ì •ì‹œê°„: {stored_modified_time}")
                logger.info(f"  - Drive ìˆ˜ì •ì‹œê°„: {modified_time}")
                logger.info(f"  - Qdrant ì €ì¥ëœ ë²„ì „: {stored_chunk_version}")
                logger.info(f"  - í˜„ì¬ ë²„ì „: {CHUNK_VERSION}")
                
                time_match = stored_modified_time == modified_time
                version_match = stored_chunk_version == CHUNK_VERSION
                
                logger.info(f"  â° ìˆ˜ì •ì‹œê°„ ì¼ì¹˜: {time_match}")
                logger.info(f"  ğŸ·ï¸ ë²„ì „ ì¼ì¹˜: {version_match}")
                
                if time_match and version_match:
                    should_process = False
                    skipped_files += 1
                    logger.info(f"  âœ… ìŠ¤í‚µ: ì´ë¯¸ ìµœì‹  ìƒíƒœ")
            else:
                logger.info(f"  - Qdrantì— ê¸°ì¡´ ë°ì´í„° ì—†ìŒ")
            
            if not should_process:
                continue
                
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
            status = "ì‹ ê·œ" if not stored_payload else "ë³€ê²½ë¨"
            logger.info(f"  ğŸ”„ ì²˜ë¦¬ í•„ìš”: '{file_name}' (ì´ìœ : {status})")
            logger.info(f"  ğŸ”„ íŒŒì¼ ë³€í™˜ ì‹œì‘...")
            
            content = download_and_parse_file(service, file_id, file_name, file_meta['mimeType'])
            
            if not content or not content.strip():
                logger.warning(f"  âš ï¸ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ì½˜í…ì¸ ")
                failed_files += 1
                continue
            
            # âœ… LangChain êµ¬ì¡°ì— ë§ì¶˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            if stored_payload:
                try:
                    delete_result = qdrant_client.delete(
                        collection_name=COLLECTION_NAME,
                        points_selector=models.FilterSelector(
                            filter=models.Filter(
                                must=[
                                    models.FieldCondition(
                                        key="metadata.file_id",  # âœ… LangChainì€ metadata. ì ‘ë‘ì‚¬ í•„ìš”
                                        match=models.MatchValue(value=file_id)
                                    )
                                ]
                            )
                        )
                    )
                    logger.info(f"  ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {delete_result}")
                except Exception as delete_error:
                    logger.error(f"  âŒ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {delete_error}")
                    failed_files += 1
                    continue
            
            # ì²­í‚¹ ë° ì €ì¥
            try:
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ
                if file_meta['mimeType'] in [
                    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                    'application/vnd.ms-excel'
                ]:
                    # ì—‘ì…€ì˜ ê²½ìš° ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„ë¦¬ì ì‚¬ìš©
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=CHUNK_SIZE, 
                        chunk_overlap=CHUNK_OVERLAP,
                        separators=["\n\n", "\n", "| ", "|", ".", " ", ""]
                    )
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ì˜ ê²½ìš° í‘œì¤€ ë¶„ë¦¬ì ì‚¬ìš©
                    splitter = RecursiveCharacterTextSplitter(
                        chunk_size=CHUNK_SIZE, 
                        chunk_overlap=CHUNK_OVERLAP,
                        separators=["\n\n", "\n", ".", "!", "?", ";", " ", ""]
                    )
                
                chunks = splitter.split_text(content)
                
                if not chunks:
                    logger.warning(f"  âš ï¸ ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                    failed_files += 1
                    continue
                
                # âœ… ë©”íƒ€ë°ì´í„°ë¥¼ Documentì˜ metadataì— ì§ì ‘ ì €ì¥
                docs_to_embed = [
                    Document(
                        page_content=chunk, 
                        metadata={
                            "source": file_name,
                            "file_id": file_id,
                            "modified_time": modified_time,
                            "chunk_version": CHUNK_VERSION,
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "mime_type": file_meta['mimeType'],
                            "file_type": get_file_type_category(file_meta['mimeType'])
                        }
                    ) for i, chunk in enumerate(chunks) if chunk.strip()
                ]
                
                if not docs_to_embed:
                    logger.warning(f"  âš ï¸ ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŒ")
                    failed_files += 1
                    continue
                
                # âœ… ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥
                for i in range(0, len(docs_to_embed), EMBEDDING_BATCH_SIZE):
                    batch = docs_to_embed[i:i + EMBEDDING_BATCH_SIZE]
                    try:
                        vectorstore.add_documents(batch)
                        logger.info(f"    ğŸ“¦ ë°°ì¹˜ {i//EMBEDDING_BATCH_SIZE + 1}/{(len(docs_to_embed)-1)//EMBEDDING_BATCH_SIZE + 1} ì €ì¥ ì™„ë£Œ ({len(batch)}ê°œ ì²­í¬)")
                    except Exception as batch_error:
                        logger.error(f"    âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {batch_error}")
                        raise
                
                avg_chunk_size = sum(len(doc.page_content) for doc in docs_to_embed) // len(docs_to_embed)
                logger.info(f"  âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(docs_to_embed)}ê°œ ì²­í¬ ìƒì„±")
                logger.info(f"     ğŸ“Š í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size}ì")
                
                processed_files += 1
                
            except Exception as e:
                logger.error(f"  âŒ ë²¡í„° ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
                failed_files += 1
        
        logger.info(f"ğŸ‰ ë™ê¸°í™” ì™„ë£Œ: ì²˜ë¦¬ {processed_files}ê°œ, ê±´ë„ˆëœ€ {skipped_files}ê°œ, ì‹¤íŒ¨ {failed_files}ê°œ")
    
    except Exception as e:
        logger.error(f"ğŸ’¥ ë™ê¸°í™” ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
    finally:
        sync_in_progress = False

def get_file_type_category(mime_type):
    """íŒŒì¼ íƒ€ì… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    if mime_type in ['application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'application/vnd.ms-excel']:
        return "excel"
    elif mime_type == 'application/pdf':
        return "pdf"
    elif mime_type == 'text/csv':
        return "csv"
    elif mime_type == 'application/json':
        return "json"
    elif mime_type == 'application/vnd.google-apps.document':
        return "google_doc"
    elif mime_type.startswith('text/'):
        return "text"
    else:
        return "other"
# --- 6. FastAPI ìˆ˜ëª…ì£¼ê¸° (ìˆ˜ì •ëœ ë²„ì „) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global vectorstore, qa_chain, embeddings, scheduler
    logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘...")

    # --- 1. RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ---
    try:
        # LangChainDeprecationWarning í•´ê²°: ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
        embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)
        # ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ë²¡í„° ì°¨ì› í™•ì¸
        test_embedding = embeddings.embed_query("test")
        vector_size = len(test_embedding)
        logger.info(f"Ollama ì„ë² ë”© ëª¨ë¸({EMBEDDING_MODEL}) ë¡œë“œ ì™„ë£Œ. ë²¡í„° ì°¨ì›: {vector_size}")
    except Exception as e:
        logger.critical(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        raise RuntimeError("Embedding model initialization failed") from e

    # --- 2. Qdrant í´ë¼ì´ì–¸íŠ¸ ë° ì»¬ë ‰ì…˜ ì„¤ì • (ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì • ë¶€ë¶„) ---
    try:
        client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ë” ì•ˆì •ì ì¸ ë°©ì‹ìœ¼ë¡œ í™•ì¸
        collections_response = client.get_collections()
        existing_collections = {c.name for c in collections_response.collections}
        collection_exists = COLLECTION_NAME in existing_collections

        if collection_exists:
            info = client.get_collection(collection_name=COLLECTION_NAME)
            current_vector_size = info.config.params.vectors.size
            logger.info(f"ğŸ“Š ê¸°ì¡´ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ë°œê²¬ - DB ë²¡í„° ì°¨ì›: {current_vector_size}, í˜„ì¬ ëª¨ë¸ ì°¨ì›: {vector_size}")

            # ë²¡í„° ì°¨ì›ì´ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì¬ìƒì„± (í™˜ê²½ ë³€ìˆ˜ ì²´í¬ í¬í•¨)
            force_skip = os.getenv("FORCE_SKIP_RECREATE", "false").lower() == "true"
            if current_vector_size != vector_size and not force_skip:
                logger.warning(f"âš ï¸ ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜! ì»¬ë ‰ì…˜ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë°ì´í„° ì‚­ì œë¨)")
                client.recreate_collection(
                    collection_name=COLLECTION_NAME,
                    vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)
                )
            elif force_skip and current_vector_size != vector_size:
                 logger.warning(f"ğŸ”’ ê°•ì œ ë³´ì¡´ ëª¨ë“œ: ë²¡í„° ì°¨ì›ì´ ë‹¤ë¥´ì§€ë§Œ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
            else:
                logger.info("âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì¬ì‚¬ìš©.")
        else:
            # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ ìƒˆë¡œ ìƒì„± (ë°ì´í„° ë³´ì¡´)
            logger.info(f"ğŸ“ ìƒˆ ì»¬ë ‰ì…˜ '{COLLECTION_NAME}' ìƒì„±")
            client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)
            )

    except Exception as e:
        logger.critical(f"ğŸ’¥ Qdrant ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise RuntimeError("Failed to initialize Qdrant collection") from e

    # LangChainDeprecationWarning í•´ê²°: ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
    vectorstore = Qdrant(
        client=client, 
        collection_name=COLLECTION_NAME, 
        embeddings=embeddings
    )
    llm = OllamaLLM(model=LLM_MODEL, base_url=OLLAMA_BASE_URL)
    
    # --- 3. QA ì²´ì¸ ë° Retriever ì„¤ì • ---
    retriever = vectorstore.as_retriever(
        search_type="similarity", 
        search_kwargs={'k': 8}
    )
    
    improved_prompt = PromptTemplate(
        template="""ë‹¹ì‹ ì€ ì—…ë¬´ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë‹µë³€ ê·œì¹™:**
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì—ì„œë§Œ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
4. í‘œë‚˜ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš° êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ í•­ëª©ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”

**ë¬¸ì„œ ë‚´ìš©:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**""",
        input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": improved_prompt},
        return_source_documents=True
    )
    logger.info("ğŸ¯ RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")

    # --- 4. ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (ìˆ˜ì •ëœ ë¶€ë¶„) ---
    if not scheduler.running:
        # ë‹¨ì¼ ì‘ì—…ìœ¼ë¡œ í†µí•©í•˜ê³ , ì²« ì‹¤í–‰ì— 10ì´ˆ ì§€ì—°ì„ ì¤Œ
        scheduler.add_job(
            perform_sync,
            'interval',
            minutes=SYNC_INTERVAL_MINUTES,
            id="gdrive_sync_job",  # ë‹¨ì¼ ID ì‚¬ìš©
            next_run_time=datetime.now() + timedelta(seconds=10) # ì¬ì‹œì‘ ì‹œ ì•ˆì •ì„±ì„ ìœ„í•œ ì§€ì—°
        )
        scheduler.start()
        logger.info(f"ìë™ ë™ê¸°í™” ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘. 10ì´ˆ í›„ ì²« ë™ê¸°í™”ë¥¼ ì‹œì‘í•˜ë©°, ì´í›„ {SYNC_INTERVAL_MINUTES}ë¶„ë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    yield

    logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ...")
    if scheduler.running: 
        scheduler.shutdown()
        
# --- 7. FastAPI ì•± ë° API ---
app = FastAPI(title="Optimized Google Drive RAG Application", version="6.0.0-optimized", lifespan=lifespan)

class QueryRequest(BaseModel):
    question: str = Field(..., min_length=3, max_length=500, title="ì§ˆë¬¸", description="RAG ëª¨ë¸ì—ê²Œ í•  ì§ˆë¬¸")

class QueryResponse(BaseModel):
    question: str
    answer: str
    sources_used: int = None
    file_types_used: List[str] = []

@app.get("/")
async def root():
    return {"message": "Optimized Google Drive RAG APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. /docs ì—ì„œ API ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."}

@app.post("/query", response_model=QueryResponse)
async def query_documents(req: QueryRequest):
    """ê°œì„ ëœ ì¿¼ë¦¬ ì²˜ë¦¬"""
    if not qa_chain:
        raise HTTPException(status_code=503, detail="QA ì²´ì¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        question = req.question.strip()
        logger.info(f"ğŸ” ì§ˆë¬¸: {question}")
        
        result = qa_chain.invoke({"query": question})
        
        # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
        source_docs = result.get('source_documents', [])
        sources_used = len(source_docs)
        
        # ì‚¬ìš©ëœ íŒŒì¼ íƒ€ì… ë¶„ì„
        file_types_used = list(set(
            doc.metadata.get('file_type', 'unknown') 
            for doc in source_docs 
            if doc.metadata.get('file_type')
        ))
        
        return {
            "question": question,
            "answer": result['result'].strip(),
            "sources_used": sources_used,
            "file_types_used": file_types_used
        }
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@app.post("/manual-sync", status_code=202)
async def manual_sync(background_tasks: BackgroundTasks):
    background_tasks.add_task(perform_sync)
    return {"message": "ìˆ˜ë™ ë™ê¸°í™” ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": "6.0.0-optimized",
        "drive_service": "connected" if drive_service else "not_connected",
        "rag_components_initialized": qa_chain is not None,
        "scheduler_running": scheduler.running if scheduler else False,
        "chunk_version": CHUNK_VERSION,
        "embedding_model": EMBEDDING_MODEL,
        "llm_model": LLM_MODEL,
        "sync_in_progress": sync_in_progress
    }

@app.get("/debug/search")
async def debug_search(query: str, limit: int = 5):
    """ê²€ìƒ‰ ê²°ê³¼ ë””ë²„ê¹…ìš© ì—”ë“œí¬ì¸íŠ¸"""
    if not vectorstore:
        raise HTTPException(status_code=503, detail="Vector store not initialized")
    
    try:
        docs = vectorstore.similarity_search(query, k=limit)
        
        return {
            "query": query,
            "total_results": len(docs),
            "results": [
                {
                    "content": doc.page_content[:300] + "...",
                    "metadata": doc.metadata,
                    "content_length": len(doc.page_content),
                    "file_type": doc.metadata.get('file_type', 'unknown')
                }
                for doc in docs
            ]
        }
    except Exception as e:
        logger.error(f"ë””ë²„ê·¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug/collection-info")
async def collection_info():
    """ìˆ˜ì •ëœ Qdrant ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸"""
    if not vectorstore:
        raise HTTPException(status_code=503, detail="Vector store not initialized")
    
    try:
        client = vectorstore.client
        info = client.get_collection(COLLECTION_NAME)
        
        # íŒŒì¼ë³„ í†µê³„ ê³„ì‚°
        file_stats = {}
        file_type_stats = {}
        
        all_points, _ = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=10000,
            with_payload=True,
            with_vectors=False
        )
        
        for point in all_points:
            file_id = point.payload.get('metadata', {}).get('file_id', 'unknown')
            source = point.payload.get('metadata', {}).get('source', 'unknown')
            file_type = point.payload.get('metadata', {}).get('file_type', 'unknown')
            
            # íŒŒì¼ë³„ í†µê³„
            if file_id not in file_stats:
                metadata = point.payload.get('metadata', {})
                file_stats[file_id] = {
                    'source': source,
                    'chunk_count': 0,
                    'modified_time': metadata.get('modified_time'),
                    'chunk_version': metadata.get('chunk_version'),
                    'file_type': file_type
                }
            file_stats[file_id]['chunk_count'] += 1
            
            # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
            if file_type not in file_type_stats:
                file_type_stats[file_type] = {'count': 0, 'files': 0}
            file_type_stats[file_type]['count'] += 1
            if file_id not in [f['file_id'] for f in file_type_stats[file_type].get('file_list', [])]:
                file_type_stats[file_type]['files'] += 1
        
        # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
        sample_points, _ = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=5,
            with_payload=True,
            with_vectors=False
        )
        
        return {
            "collection_name": COLLECTION_NAME,
            "total_points": info.points_count,
            "vector_size": info.config.params.vectors.size,
            "distance_metric": info.config.params.vectors.distance.value,
            "total_files": len(file_stats),
            "file_type_statistics": file_type_stats,
            "file_statistics": file_stats,
            "sample_points": [
                {
                    "id": point.id,
                    "file_id": point.payload.get('metadata', {}).get('file_id', 'unknown'),
                    "source": point.payload.get('metadata', {}).get('source', 'unknown'),
                    "file_type": point.payload.get('metadata', {}).get('file_type', 'unknown'),
                    "modified_time": point.payload.get('metadata', {}).get('modified_time'),
                    "chunk_version": point.payload.get('metadata', {}).get('chunk_version'),
                    "chunk_index": point.payload.get('metadata', {}).get('chunk_index'),
                    "content_preview": point.payload.get('page_content', '')[:200] + "..."
                }
                for point in sample_points
            ]
        }
    except Exception as e:
        logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug/file-check/{file_id}")
async def check_file_exists(file_id: str):
    """íŠ¹ì • íŒŒì¼ì˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    if not vectorstore:
        raise HTTPException(status_code=503, detail="Vector store not initialized")
    
    try:
        client = vectorstore.client
        
        # âœ… LangChain êµ¬ì¡°ì— ë§ì¶˜ ê²€ìƒ‰
        existing, _ = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=100,
            with_payload=True,
            with_vectors=False,
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="metadata.file_id",  # âœ… LangChainì€ metadata. ì ‘ë‘ì‚¬ í•„ìš”
                        match=models.MatchValue(value=file_id)
                    )
                ]
            )
        )
        
        if existing:
            chunks_info = []
            for point in existing:
                chunks_info.append({
                    "chunk_index": point.payload.get('chunk_index'),
                    "content_length": len(point.payload.get('page_content', '')),
                    "content_preview": point.payload.get('page_content', '')[:100] + "..."
                })
            
            return {
                "file_id": file_id,
                "exists": True,
                "total_chunks": len(existing),
                "file_info": {
                    "source": existing[0].payload.get('metadata', {}).get('source'),
                    "modified_time": existing[0].payload.get('metadata', {}).get('modified_time'),
                    "chunk_version": existing[0].payload.get('metadata', {}).get('chunk_version'),
                    "file_type": existing[0].payload.get('metadata', {}).get('file_type')
                },
                "chunks": chunks_info
            }
        else:
            return {
                "file_id": file_id,
                "exists": False,
                "total_chunks": 0
            }
            
    except Exception as e:
        logger.error(f"íŒŒì¼ ì¡´ì¬ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)