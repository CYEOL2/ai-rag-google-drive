# app.py (Final Corrected Version)

import uvicorn
import os
import logging
from pathlib import Path
from fastapi import FastAPI, HTTPException, BackgroundTasks
from contextlib import asynccontextmanager
import uuid
from pydantic import BaseModel, Field

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import OllamaEmbeddings

# ëŒ€ì²´ import ì‹œë„
try:
    from langchain_qdrant import Qdrant
except ImportError:
    from langchain_community.vectorstores import Qdrant
    
try:
    from langchain_ollama import OllamaLLM
except ImportError:
    from langchain_community.llms import Ollama as OllamaLLM

# Google Drive API ê´€ë ¨
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseDownload

# ê¸°íƒ€
import io
import pandas as pd
from qdrant_client import QdrantClient, models
from apscheduler.schedulers.background import BackgroundScheduler

# --- 1. ë¡œê¹… ë° ê¸°ë³¸ ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 2. í™˜ê²½ ë³€ìˆ˜ ë° ì£¼ìš” ì„¤ì •ê°’ ---
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "llama3.2")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.2")
COLLECTION_NAME = "gdrive_rag_collection"
SERVICE_ACCOUNT_FILE = "/app/service_account.json"
GOOGLE_DRIVE_FOLDER_ID = os.getenv("GOOGLE_DRIVE_FOLDER_ID")
SYNC_INTERVAL_MINUTES = int(os.getenv("SYNC_INTERVAL_MINUTES", "60"))
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# RAG ì²˜ë¦¬ ê´€ë ¨ ì„¤ì • - Excelì— ìµœì í™”
CHUNK_SIZE = 2000  # ì—…ë¬´ê³µìœ  ë¬¸ì„œ íŠ¹ì„±ìƒ ë” í° ì²­í¬
CHUNK_OVERLAP = 300  # ë” ë§ì€ ì˜¤ë²„ë©ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´
EMBEDDING_BATCH_SIZE = 128
CHUNK_VERSION = f"{CHUNK_SIZE}_{CHUNK_OVERLAP}_excel_optimized"

# --- 3. ì „ì—­ ë³€ìˆ˜ ---
vectorstore = None
qa_chain = None
embeddings = None
scheduler = BackgroundScheduler(timezone="Asia/Seoul")
drive_service = None

# --- 4. Google Drive ì—°ë™ ---
def get_drive_service():
    global drive_service
    if drive_service: return drive_service
    if not Path(SERVICE_ACCOUNT_FILE).exists():
        logger.error(f"ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼({SERVICE_ACCOUNT_FILE})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    try:
        credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
        drive_service = build('drive', 'v3', credentials=credentials)
        logger.info("Google Drive API ì„œë¹„ìŠ¤ ì¸ì¦ ì™„ë£Œ")
        return drive_service
    except Exception as e:
        logger.error(f"Google Drive API ì„œë¹„ìŠ¤ ì¸ì¦ ì‹¤íŒ¨: {e}")
        return None

def _decode_bytes_to_text(content_bytes, file_id):
    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'iso-8859-1', 'latin1']
    for encoding in encodings:
        try:
            return content_bytes.decode(encoding)
        except UnicodeDecodeError:
            continue
    logger.warning(f"íŒŒì¼ {file_id} ë””ì½”ë”© ì‹¤íŒ¨. ì¼ë¶€ ë¬¸ì ëŒ€ì²´ë¨.")
    return content_bytes.decode('utf-8', errors='replace')

def download_file_content(service, file_id, file_name, mime_type):
    request = None
    if mime_type == 'application/vnd.google-apps.document':
        request = service.files().export_media(fileId=file_id, mimeType='text/plain')
    elif mime_type in ['application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'application/vnd.ms-excel']:
        request = service.files().get_media(fileId=file_id)
    elif mime_type.startswith('text/'):
        request = service.files().get_media(fileId=file_id)
    else:
        logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” MIME íƒ€ì…({mime_type})ìœ¼ë¡œ íŒŒì¼ '{file_name}'ì„(ë¥¼) ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    try:
        file_io = io.BytesIO()
        downloader = MediaIoBaseDownload(file_io, request)
        done = False
        while not done: _, done = downloader.next_chunk()
        file_io.seek(0)

        # ì—‘ì…€ íŒŒì¼ì¸ ê²½ìš°, ì—…ë¬´ê³µìœ  ë¬¸ì„œì— íŠ¹í™”ëœ íŒŒì‹±
        if mime_type in ['application/vnd.openxmlformats-officedocument.spreadsheetml.sheet', 'application/vnd.ms-excel']:
            try:
                xls = pd.ExcelFile(file_io, engine='openpyxl')
                parts = []
                logger.info(f"ì—‘ì…€ íŒŒì¼ '{file_name}' ì²˜ë¦¬ ì‹œì‘. ì‹œíŠ¸ ëª©ë¡: {xls.sheet_names}")
                
                for sheet in xls.sheet_names:
                    # ëª¨ë“  ì…€ì„ ë¬¸ìì—´ë¡œ ì½ê³  ë³‘í•©ëœ ì…€ ì •ë³´ ìœ ì§€
                    df = pd.read_excel(xls, sheet_name=sheet, header=None, dtype=str, keep_default_na=False)
                    
                    if df.empty: 
                        continue
                    
                    # ì‹œíŠ¸ í—¤ë”
                    header = f"\n{'='*60}\n"
                    header += f"ğŸ“‹ ì—‘ì…€ ì‹œíŠ¸: {sheet}\n"
                    header += f"{'='*60}\n\n"
                    
                    # ì…€ë³„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ (í–‰ë ¬ êµ¬ì¡° ë³´ì¡´)
                    content_sections = []
                    
                    for row_idx, row in df.iterrows():
                        row_content = []
                        for col_idx, cell_value in enumerate(row):
                            if pd.notna(cell_value) and str(cell_value).strip():
                                clean_val = str(cell_value).strip()
                                if clean_val and clean_val.lower() not in ['nan', 'none', 'unnamed']:
                                    row_content.append(f"[{row_idx+1},{col_idx+1}] {clean_val}")
                        
                        if row_content:
                            content_sections.append("\n".join(row_content))
                    
                    # í”„ë¡œì íŠ¸ë³„ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    all_text = " ".join([str(cell) for row in df.values for cell in row if pd.notna(cell)])
                    
                    # ì£¼ìš” ì •ë³´ íŒ¨í„´ ê²€ìƒ‰ (ë²”ìš©ì )
                    project_info = "\nğŸ” ì£¼ìš” ì •ë³´:\n"
                    
                    # í”„ë¡œì íŠ¸/ì‚¬ì—… ê´€ë ¨ íŒ¨í„´ ê²€ìƒ‰
                    import re
                    project_patterns = re.findall(r'[ê°€-í£\w\s]*(?:í”„ë¡œì íŠ¸|ì‚¬ì—…|ì‹œìŠ¤í…œ|ê°œë°œ|ê³ ë„í™”)[ê°€-í£\w\s]*', all_text, re.IGNORECASE)
                    if project_patterns:
                        unique_projects = list(set([p.strip() for p in project_patterns if len(p.strip()) > 2]))
                        project_info += "\nğŸ“Œ í”„ë¡œì íŠ¸/ì‚¬ì—…:\n"
                        for pattern in unique_projects[:10]:
                            project_info += f"  - {pattern}\n"
                    
                    # ì¸ë ¥/ë‹´ë‹¹ì ê´€ë ¨ íŒ¨í„´ ê²€ìƒ‰
                    manpower_patterns = re.findall(r'(?:PL|íˆ¬ì…|ì¸ë ¥|ë‹´ë‹¹)[^\n]*(?:ëª…|ê°œì›”|ë…„)[^\n]*', all_text, re.IGNORECASE)
                    if manpower_patterns:
                        project_info += "\nğŸ‘¥ ì¸ë ¥ ì •ë³´:\n"
                        for pattern in manpower_patterns:
                            project_info += f"  - {pattern.strip()}\n"
                    
                    # ë‹´ë‹¹ìëª… íŒ¨í„´ ê²€ìƒ‰
                    name_patterns = re.findall(r'[ê°€-í£]{2,4}\s*(?:ë¶€ì¥|íŒ€ì¥|ì—°êµ¬ì›|ê°œë°œì|ëŒ€ë¦¬|ê³¼ì¥|ì°¨ì¥)?', all_text)
                    if name_patterns:
                        unique_names = list(set([name.strip() for name in name_patterns if len(name.strip()) >= 2]))
                        if unique_names:
                            project_info += f"\nğŸ‘¤ ê´€ë ¨ ì¸ëª…: {', '.join(unique_names[:15])}\n"
                    
                    # ì¼ì • ì •ë³´ ê²€ìƒ‰
                    schedule_patterns = re.findall(r'\d{4}[.-]\d{1,2}[.-]\d{1,2}|\d{1,2}ì›”\s*ì°©ìˆ˜|\d{1,2}ê°œì›”|\d{4}ë…„\s*\d{1,2}ì›”', all_text)
                    if schedule_patterns:
                        project_info += f"\nğŸ“… ì¼ì • ì •ë³´: {', '.join(set(schedule_patterns[:10]))}\n"
                    
                    # ìµœì¢… ì»¨í…ì¸  ì¡°í•©
                    sheet_content = header + "\n".join(content_sections[:50]) + project_info  # ìµœëŒ€ 50ê°œ ì„¹ì…˜
                    parts.append(sheet_content)
                    logger.info(f"ì‹œíŠ¸ '{sheet}' ì²˜ë¦¬ ì™„ë£Œ: {len(content_sections)}ê°œ ì„¹ì…˜")
                
                result = "\n\n".join(parts) if parts else None
                if result:
                    logger.info(f"ì—‘ì…€ íŒŒì¼ '{file_name}' ì²˜ë¦¬ ì™„ë£Œ. ì´ {len(parts)}ê°œ ì‹œíŠ¸, ê¸¸ì´: {len(result)}ì")
                return result
            except Exception as excel_error:
                logger.error(f"ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file_name}): {excel_error}")
                return None

        # ê·¸ ì™¸ í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼ ì²˜ë¦¬
        return _decode_bytes_to_text(file_io.getvalue(), file_id)
    except Exception as e:
        logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨ ({file_name}): {e}")
        return None

def get_files_from_drive(folder_id):
    service = get_drive_service()
    if not service: return []
    try:
        query = f"'{folder_id}' in parents and trashed=false"
        results = service.files().list(q=query, fields="files(id, name, modifiedTime, mimeType)", pageSize=1000).execute()
        docs = []
        for file in results.get('files', []):
            logger.info(f"ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ë°œê²¬: {file['name']} (ID: {file['id']})")
            if content := download_file_content(service, file['id'], file['name'], file['mimeType']):
                docs.append({'id': file['id'], 'name': file['name'], 'content': content, 'modified_time': file['modifiedTime']})
                logger.info(f"íŒŒì¼ ì½˜í…ì¸  ë¡œë“œ ì™„ë£Œ: {file['name']}")
        return docs
    except HttpError as e:
        logger.error(f"Google Drive API ì˜¤ë¥˜: {e}")
        return []

# --- 5. ë™ê¸°í™” ë¡œì§ (íŒŒì¼ ìˆ˜ì •ì¼ì‹œ ê¸°ì¤€ ì¤‘ë³µ ë°©ì§€) ---
def perform_sync():
    global embeddings, vectorstore
    if not all([embeddings, vectorstore]) or not GOOGLE_DRIVE_FOLDER_ID:
        logger.warning("RAG ì»´í¬ë„ŒíŠ¸ ë¯¸ì´ˆê¸°í™” ë˜ëŠ” í´ë” ID ëˆ„ë½ìœ¼ë¡œ ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    logger.info(f"--- ë™ê¸°í™” ì‹œì‘ (í´ë” ID: {GOOGLE_DRIVE_FOLDER_ID}) ---")
    documents = get_files_from_drive(GOOGLE_DRIVE_FOLDER_ID)
    if not documents:
        logger.info("ë™ê¸°í™”í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    qdrant_client = vectorstore.client
    processed_files, skipped_files = 0, 0
    drive_file_ids = {doc['id'] for doc in documents}

    for doc in documents:
        file_id, file_name, modified_time, content = doc['id'], doc['name'], doc['modified_time'], doc['content']
        
        # Vector DBì—ì„œ í•´ë‹¹ íŒŒì¼ì˜ ìµœì‹  ë©”íƒ€ë°ì´í„° í™•ì¸
        existing, _ = qdrant_client.scroll(
            collection_name=COLLECTION_NAME, limit=1, with_vectors=False,
            scroll_filter=models.Filter(must=[models.FieldCondition(key="file_id", match=models.MatchValue(value=file_id))]),
            with_payload=["modified_time", "chunk_version"]
        )
        stored_payload = existing[0].payload if existing else None
        
        # ì¡°ê±´ í™•ì¸: íŒŒì¼ ìˆ˜ì •ì¼ì‹œì™€ ì²­í‚¹ ë²„ì „ì´ ëª¨ë‘ ë™ì¼í•œ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if (stored_payload and
            stored_payload.get("modified_time") == modified_time and
            stored_payload.get("chunk_version") == CHUNK_VERSION):
            skipped_files += 1
            continue

        status = "New"
        if stored_payload:
            if stored_payload.get("chunk_version") != CHUNK_VERSION:
                status = "Update (Chunk Strategy Changed)"
            else:
                status = "Update (File Modified)"
        
        logger.info(f"  - [{status}] íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_name}")
        
        # íŒŒì¼ ë‚´ìš© ë¶„í•  (Chunking)
        splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
        chunks = splitter.split_text(content)
        metadata = {"source": file_name, "file_id": file_id, "modified_time": modified_time, "chunk_version": CHUNK_VERSION}
        docs_to_embed = [Document(page_content=t, metadata=metadata) for t in chunks]
        
        # ì„ë² ë”© ë° Vector DB ì €ì¥ì„ ìœ„í•œ ë°ì´í„° ìƒì„±
        points, batch_failed = [], False
        for i in range(0, len(docs_to_embed), EMBEDDING_BATCH_SIZE):
            batch = docs_to_embed[i:i + EMBEDDING_BATCH_SIZE]
            try:
                vectors = embeddings.embed_documents([d.page_content for d in batch])
                for k, doc_chunk in enumerate(batch):
                    pid = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{file_id}-{i+k}"))
                    payload = {**doc_chunk.metadata, 'page_content': doc_chunk.page_content}
                    points.append(models.PointStruct(id=pid, vector=vectors[k], payload=payload))
            except Exception as e:
                logger.error(f"ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_name}, ì‹œì‘ ì¸ë±ìŠ¤: {i}): {e}")
                batch_failed = True; break
        if batch_failed: continue
        
        # ì—…ë°ì´íŠ¸ì¸ ê²½ìš°, ê¸°ì¡´ ë°ì´í„° ë¨¼ì € ì‚­ì œ
        if status.startswith("Update"):
            logger.debug(f"ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘: {file_name}")
            qdrant_client.delete(
                collection_name=COLLECTION_NAME,
                points_selector=models.FilterSelector(filter=models.Filter(must=[models.FieldCondition(key="file_id", match=models.MatchValue(value=file_id))]))
            )

        if points:
            qdrant_client.upsert(collection_name=COLLECTION_NAME, points=points, wait=True)
        processed_files += 1

    # Google Driveì—ì„œ ì‚­ì œëœ íŒŒì¼ë“¤ì„ Vector DBì—ì„œ ì œê±°
    logger.info("ì‚­ì œëœ íŒŒì¼ í™•ì¸ ë° ì •ë¦¬ ì‹œì‘...")
    qdrant_file_ids = set()
    next_offset = None
    while True:
        points, next_offset = qdrant_client.scroll(
            collection_name=COLLECTION_NAME, limit=256, offset=next_offset,
            with_payload=['file_id'], with_vectors=False
        )
        for point in points: qdrant_file_ids.add(point.payload['file_id'])
        if not next_offset: break
    
    ids_to_delete = qdrant_file_ids - drive_file_ids
    if ids_to_delete:
        logger.info(f"ì´ {len(ids_to_delete)}ê°œì˜ ì‚­ì œëœ íŒŒì¼ì„ Qdrantì—ì„œ ì œê±°í•©ë‹ˆë‹¤.")
        qdrant_client.delete(
            collection_name=COLLECTION_NAME,
            points_selector=models.FilterSelector(filter=models.Filter(
                must=[models.FieldCondition(key="file_id", match=models.MatchAny(any=list(ids_to_delete)))]
            ))
        )
    
    logger.info(f"--- ë™ê¸°í™” ì™„ë£Œ: ì²˜ë¦¬ {processed_files}ê°œ, ê±´ë„ˆëœ€ {skipped_files}ê°œ, ì‚­ì œ {len(ids_to_delete)}ê°œ (Chunk Version: {CHUNK_VERSION}) ---")

# --- 6. FastAPI ìˆ˜ëª…ì£¼ê¸° (Lifespan) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global vectorstore, qa_chain, embeddings, scheduler
    logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘...")

    try:
        embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_BASE_URL)
        vector_size = len(embeddings.embed_query("test"))
        logger.info(f"Ollama ì„ë² ë”© ëª¨ë¸({EMBEDDING_MODEL}) ë¡œë“œ ì™„ë£Œ. ë²¡í„° ì°¨ì›: {vector_size}")
    except Exception as e:
        logger.critical(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨! ì•±ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
        raise RuntimeError("Embedding model initialization failed") from e

    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    try:
        info = client.get_collection(collection_name=COLLECTION_NAME)
        current_vector_size = info.vectors_config.params.size
        if current_vector_size != vector_size:
            logger.warning(f"ì»¬ë ‰ì…˜ì˜ ë²¡í„° ì°¨ì›({current_vector_size})ì´ ëª¨ë¸({vector_size})ê³¼ ë‹¬ë¼ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
            client.recreate_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)
            )
    except Exception:
        logger.info(f"ì»¬ë ‰ì…˜ '{COLLECTION_NAME}'ì„(ë¥¼) ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        client.recreate_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE)
        )

    vectorstore = Qdrant(client=client, collection_name=COLLECTION_NAME, embeddings=embeddings)
    llm = OllamaLLM(model=LLM_MODEL, base_url=OLLAMA_BASE_URL)
    
    # Excel ë°ì´í„°ì— ìµœì í™”ëœ retriever ì„¤ì •
    retriever = vectorstore.as_retriever(
        search_type="similarity", 
        search_kwargs={'k': 12}  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    )
    
    # Excel ë¬¸ì„œ í•´ì„ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸
    custom_prompt = PromptTemplate(
        template="""ë‹¹ì‹ ì€ ì—‘ì…€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì¤‘ìš” ì§€ì¹¨]
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ì •í™•íˆ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì…€ ìœ„ì¹˜ ì •ë³´ [í–‰,ì—´]ì„ ì°¸ê³ í•˜ì—¬ ê´€ë ¨ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì„¸ìš”
3. í”„ë¡œì íŠ¸ëª…, ë‹´ë‹¹ì, ì¸ë ¥, ì¼ì • ë“± êµ¬ì²´ì  ì •ë³´ë¥¼ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
4. ì—¬ëŸ¬ ì‹œíŠ¸ì— ê±¸ì¹œ ê´€ë ¨ ì •ë³´ê°€ ìˆë‹¤ë©´ ëª¨ë‘ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
5. ë² íŠ¸ë‚¨ì–´ë‚˜ ê¸°íƒ€ ì–¸ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³  ì˜¤ì§ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”
6. ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¬¸ì„œì— ëª…í™•íˆ ê¸°ë¡ëœ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”

[ë¶„ì„í•  ë¬¸ì„œ ë‚´ìš©]
{context}

[ì§ˆë¬¸]
{question}

[ì •í™•í•œ í•œêµ­ì–´ ë‹µë³€]""",
        input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=retriever,
        chain_type_kwargs={"prompt": custom_prompt}
    )
    logger.info("RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")

    # ì•± ì‹œì‘ ì‹œ ì¦‰ì‹œ 1íšŒ ë™ê¸°í™”, ì´í›„ ì£¼ê¸°ì ìœ¼ë¡œ ë™ê¸°í™”
    if not scheduler.running:
        scheduler.add_job(perform_sync, 'date', id="gdrive_initial_sync")
        scheduler.add_job(perform_sync, 'interval', minutes=SYNC_INTERVAL_MINUTES, id="gdrive_sync_job")
        scheduler.start()
        logger.info(f"ìë™ ë™ê¸°í™” ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘. ìµœì´ˆ ì‹¤í–‰ í›„ {SYNC_INTERVAL_MINUTES}ë¶„ë§ˆë‹¤ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    yield

    logger.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ...")
    if scheduler.running: scheduler.shutdown()

# --- 7. FastAPI ì•± ë° API ---
app = FastAPI(title="Google Drive RAG Application", version="1.5.0-fixed", lifespan=lifespan)

class QueryRequest(BaseModel):
    question: str = Field(..., min_length=3, title="ì§ˆë¬¸", description="RAG ëª¨ë¸ì—ê²Œ í•  ì§ˆë¬¸")

@app.get("/")
async def root():
    return {"message": "Google Drive RAG APIê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. /docs ì—ì„œ API ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."}

@app.post("/query")
async def query_documents(req: QueryRequest):
    if not qa_chain:
        raise HTTPException(status_code=503, detail="QA ì²´ì¸ì´ ì•„ì§ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    try:
        question = req.question.strip()
        result = qa_chain.invoke({"query": question})
        
        # ê¹”ë”í•œ ì‘ë‹µ (source_documents ë°°ì—´ ì œê±°)
        return {
            "question": question, 
            "answer": result['result'].strip()
        }
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

@app.post("/manual-sync", status_code=202)
async def manual_sync(background_tasks: BackgroundTasks):
    background_tasks.add_task(perform_sync)
    return {"message": "ìˆ˜ë™ ë™ê¸°í™” ì‘ì—…ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "drive_service": "connected" if drive_service else "not_connected",
        "rag_components_initialized": qa_chain is not None,
        "scheduler_running": scheduler.running if scheduler else False
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)